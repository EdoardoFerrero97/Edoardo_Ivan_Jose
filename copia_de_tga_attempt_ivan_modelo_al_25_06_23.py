# -*- coding: utf-8 -*-
"""Copia de TGA_attempt_Ivan_modelo al 25.06.23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWSbTOZiiNf8iliq1O9YAFkWQFFrtTar
"""

#install pycaret
!pip install pycaret

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = 'drive/MyDrive/Impact Project 2023/TG-A-Ejemplo1.xlsx'
df = pd.read_excel(file_path)
df

df.head()

column_types = df.dtypes
print(column_types)

for column in df.columns:
    unique_values = df[column].unique()
    unique_count = len(unique_values)
    print(f"Unique values for {column} ({unique_count} unique):")
    print(unique_values)
    print()

column_name = 'Central-Turbina'
df = df.drop(column_name, axis=1)

column_name = 'P Activa'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'Reactiva'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'Intensidad exc'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'TensiÃ³n exc'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'TENS GENER. L1-L2'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'TENS GENER. L2-L3'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'TENS GENER. L3-L1'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'CORRIENTE GENER. L1'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'CORRIENTE GENER. L2'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_name = 'CORRIENTE GENER. L3'
df[column_name] = pd.to_numeric(df[column_name], errors='coerce')

column_types = df.dtypes
print(column_types)

df

correlation_matrix = df.corr()
correlation_matrix

# Define a new list of row names
new_column_names = ['Fecha_hora','P_Activa', 'Reactiva', 'Intensidad_exc','Tension_exc','TENS_GENER._L1-L2','TENS_GENER._L2-L3','TENS_GENER._L3-L1','CORRIENTE_GENER._L1','CORRIENTE_GENER._L2','CORRIENTE_GENER._L3']

# Assign the new row names to the DataFrame
#df.index = new_row_names
df.columns = new_column_names

df

import pandas as pd
import seaborn as sns

sns.heatmap(df.corr())

sns.heatmap(df.corr(), cmap="BuPu", linewidth=.8, annot=True)

sns.boxplot(x=df["P_Activa"])

sns.boxplot(x=df["Reactiva"])

sns.boxplot(x=df["Intensidad_exc"])

sns.boxplot(x=df["Tension_exc"])

sns.boxplot(x=df["TENS_GENER._L1-L2"])

sns.boxplot(x=df["TENS_GENER._L2-L3"])

sns.boxplot(x=df["TENS_GENER._L3-L1"])

sns.boxplot(x=df["CORRIENTE_GENER._L1"])

sns.boxplot(x=df["CORRIENTE_GENER._L2"])

sns.boxplot(x=df["CORRIENTE_GENER._L3"])

data = df.sample(frac=0.7, random_state=200)
data_unseen = df.drop(data.index)

data.reset_index(drop=True, inplace=True)
data_unseen.reset_index(drop=True, inplace=True)

print('Data: ' + str(data.shape))
print('Unseen Data: ' + str(data_unseen.shape))

from pycaret.anomaly import *

anom_exp = setup(data, normalize = True, session_id = 125, pca=True, pca_components=0.95) # pca_components = 0.95 means you want to retain 95% of the variance in the transformed dataset.

models()

#Isolation Forest
#iforest = create_model('iforest', fraction=0.025, n_estimators = 1000) # default is 100 trees # the algorithm will detect anomalies assuming to expect 2.5% of the data points to be outliers.
iforest = create_model('iforest', fraction=0.1, n_estimators = 100)

print(iforest)

#Create a K nearest neighbours model
knn = create_model('knn', fraction = 0.1, n_neighbors = 10, radius=2.0)



print(knn)

#Create a Local Outlier Factor model
lof = create_model('lof', leaf_size=20) # leaf_size: A smaller leaf size typically results in a deeper tree
                                        # structure with more nodes and lower computational efficiency. On the other
                                        # hand, a larger leaf size leads to a shallower tree with fewer nodes but
                                        # potentially higher computational efficiency.

print(lof)

#Assign model for Isolation Forest

iforest_results=assign_model(iforest)
iforest_results.head()

#number of anomalous results
iforest_results['Anomaly'].value_counts()

sns.boxplot(x=iforest_results["Anomaly_Score"])

#Assign model for KNN
knn_results = assign_model(knn)
knn_results.head()



sns.boxplot(x=knn_results["Anomaly_Score"])

# Assuming you have a list of numbers named 'numbers'
# Show the range of numbers
min_value = min(knn_results['Anomaly_Score'])
max_value = max(knn_results['Anomaly_Score'])

# Print the range
print("Minimum value:", min_value)
print("Maximum value:", max_value)

#number of anomalous results
knn_results['Anomaly'].value_counts()

#Assign model for Local Outlier Factor
lof_results=assign_model(lof)
lof_results.head()



#number of anomalous results
lof_results['Anomaly'].value_counts()

sns.boxplot(x=lof_results["Anomaly_Score"])

evaluate_model(iforest)

#palette1 = ["#0000FF", "#FF0000"]
#sns.pairplot(iforest_results, vars=["P_Activa","Reactiva","Intensidad_exc","Tension_exc","TENS_GENER._L1-L2","TENS_GENER._L2-L3","TENS_GENER._L3-L1","CORRIENTE_GENER._L3","CORRIENTE_GENER._L2","CORRIENTE_GENER._L3"], hue="Anomaly", palette=palette1)

evaluate_model(knn)

#palette2 = ["#0000FF", "#FF0000"]
#sns.pairplot(knn_results, vars=["P_Activa","Reactiva","Intensidad_exc","Tension_exc","TENS_GENER._L1-L2","TENS_GENER._L2-L3","TENS_GENER._L3-L1","CORRIENTE_GENER._L3","CORRIENTE_GENER._L2","CORRIENTE_GENER._L3"], hue="Anomaly", palette=palette2)

evaluate_model(lof)

#palette3 = ["#0000FF", "#FF0000"]
#sns.pairplot(lof_results, vars=["P_Activa","Reactiva","Intensidad_exc","Tension_exc","TENS_GENER._L1-L2","TENS_GENER._L2-L3","TENS_GENER._L3-L1","CORRIENTE_GENER._L3","CORRIENTE_GENER._L2","CORRIENTE_GENER._L3"], hue="Anomaly", palette=palette3)

#Use isolation forest to predict on unseen observations
unseen_predictions_iforest = predict_model(iforest, data=data_unseen)
unseen_predictions_iforest.head()

y_true=iforest_results['Anomaly']
df_data=iforest_results.drop('Anomaly',1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_data, y_true, test_size=0.3, random_state=42)



#Use isolation forest to predict on unseen observations
unseen_predictions_knn = predict_model(knn, data=data_unseen)
unseen_predictions_knn.head()

#Use isolation forest to predict on unseen observations
unseen_predictions_lof = predict_model(lof, data=data_unseen)
unseen_predictions_lof.head()

############################################################################################### Isolation Forest Model

# New dataset labelled ready to used in supervised model
iforest_results = iforest_results.sort_values('Fecha_hora')
iforest_results = iforest_results.reset_index()
iforest_results = iforest_results.drop('index', axis=1)
#iforest_results = iforest_results.drop('level_0', axis=1)
iforest_results

column_types = iforest_results.dtypes
print(column_types)

# Define a new list of row names
new_column_names = ['Fecha_hora','Active_Power', 'Reactive_Power', 'Excitation_Current','Excitation_Voltage','Ua','Ub','Uc','Ia','Ib','Ic','Anomaly','Anomaly_Score']

# Assign the new row names to the DataFrame
#df.index = new_row_names
iforest_results.columns = new_column_names

# Iterate over each column
for column in iforest_results.columns:
    # Calculate the mean of the column
    mean_value = iforest_results[column].mean()

    # Fill missing values with the mean value
    iforest_results[column].fillna(mean_value, inplace=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Select only the numeric columns for normalization
numeric_columns = iforest_results.select_dtypes(include=[float, int]).columns

# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize each numeric column in the DataFrame using Z-score
iforest_results_normalized = iforest_results.copy()  # Create a copy of the original DataFrame
iforest_results_normalized[numeric_columns] = scaler.fit_transform(iforest_results[numeric_columns])

# The 'anomaly' column remains unchanged
iforest_results_normalized['Anomaly'] = iforest_results['Anomaly']
iforest_results_normalized['Fecha_hora'] = iforest_results['Fecha_hora']

# linear regression does not need balanced dataset.

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score,mean_absolute_error

iforest_results['year'] = iforest_results['Fecha_hora'].dt.year
iforest_results['month'] = iforest_results['Fecha_hora'].dt.month
iforest_results['day'] = iforest_results['Fecha_hora'].dt.day
iforest_results['hour'] = iforest_results['Fecha_hora'].dt.hour
iforest_results['minute'] = iforest_results['Fecha_hora'].dt.minute

column_name = 'Fecha_hora'
iforest_results = iforest_results.drop(column_name, axis=1)

# Assuming `df` is your dataset
X1 = iforest_results.drop('Anomaly', axis=1)  # Features
y1 = iforest_results['Anomaly']  # Target variable

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# LINEAR REGRESSION MODEL

model = LinearRegression()
model.fit(X1_train, y1_train)

y1_pred_lr = model.predict(X1_test)

# Evaluate the model
mse_lr_1 = mean_squared_error(y1_test, y1_pred_lr)
r2_lr_1 = r2_score(y1_test, y1_pred_lr)
mae_lr_1 = mean_absolute_error(y1_test, y1_pred_lr)

# Print the evaluation metrics
print("Mean Absolute Error (MAE):", mae_lr_1)
print('Mean Squared Error (MSE):', mse_lr_1)
print('Coefficient of Determination (R^2):', r2_lr_1)

y1_pred_lr_binary = (y1_pred_lr >= 0.6).astype(int) # because we want to focus in reducing false negatives, we put a threshold of 0.6

# Calculate accuracy, precision and recall
accuracy_lr_1 = accuracy_score(y1_test, y1_pred_lr_binary)
precision_lr_1 = precision_score(y1_test, y1_pred_lr_binary)
recall_lr_1 = recall_score(y1_test, y1_pred_lr_binary)

print("Accuracy:", accuracy_lr_1)
print("Precision:", precision_lr_1)
print("Recall:", recall_lr_1)

import matplotlib.pyplot as plt

# Plot the predicted values against the actual values
plt.scatter(X1_test['hour'], y1_test, color='blue', label='Actual')
plt.scatter(X1_test['hour'], y1_pred_lr, color='red', label='Predicted')

# Set labels and title
plt.xlabel('Hour')
plt.ylabel('Anomaly')
plt.title('Linear Regression - Actual vs Predicted')

# Add legend
plt.legend()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import datetime

# Combine year, month, day, hour, and minute into a datetime column
iforest_results['DateTime'] = pd.to_datetime(iforest_results[['year', 'month', 'day', 'hour', 'minute']])

# Set the figure size to make the graph wider
plt.figure(figsize=(40, 2))

# Plot the predicted values against the actual values
plt.scatter(iforest_results.loc[y1_test.index, 'DateTime'], y1_test, color='blue', label='Actual')
plt.scatter(iforest_results.loc[y1_test.index, 'DateTime'], y1_pred_lr, color='red', label='Predicted')

# Set labels and title
plt.xlabel('DateTime')
plt.ylabel('Anomaly')
plt.title('Linear Regression - Actual vs Predicted')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Add legend
plt.legend()

# Display the plot
plt.show()

# RANDOM FOREST MODEL

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [50, 100, 200, 300, 400, 500],   [50-500]
#    'max_depth': [5, 10, 15, 20],                    [5-20]
#    'min_samples_split': [2, 5, 8, 10],              [1-10]
#    'min_samples_leaf': [1, 3, 2, 4, 5]              [1-5]
#}

# estimated time to run 4800 times the training process is 3 hours 20 min# (6*4*4*5*10)

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [100, 200, 300],
#    'max_depth': [5, 10, 15],
#    'min_samples_split': [2, 5, 8],
#    'min_samples_leaf': [2, 4, 5]
#}

# Grid search to find the best hyperparameters
#grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
#grid_search.fit(X1_train, y1_train)

# Best hyperparameters and best model
#best_params = grid_search.best_params_
#best_model = grid_search.best_estimator_

# Create a Random Forest classifier
random_forest = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(X1_train, y1_train)

# Predicting on the test set
y1_pred_rf = random_forest.predict(X1_test)

# Evaluate the model
mse_rf_1 = mean_squared_error(y1_test, y1_pred_rf)
r2_rf_1 = r2_score(y1_test, y1_pred_rf)
mae_rf_1 = mean_absolute_error(y1_test, y1_pred_rf)

# Print the evaluation metrics
print("Mean Absolute Error (MAE):", mae_rf_1)
print('Mean Squared Error (MSE):', mse_rf_1)
print('Coefficient of Determination (R^2):', r2_rf_1)

y1_pred_rf_binary = (y1_pred_rf >= 0.6).astype(int)

# Calculate accuracy, precision and recall
accuracy_rf_1 = accuracy_score(y1_test, y1_pred_rf_binary)
precision_rf_1 = precision_score(y1_test, y1_pred_rf_binary)
recall_rf_1 = recall_score(y1_test, y1_pred_rf_binary)

print("Accuracy:", accuracy_rf_1)
print("Precision:", precision_rf_1)
print("Recall:", recall_rf_1)

import matplotlib.pyplot as plt
import datetime

# Set the figure size to make the graph wider
plt.figure(figsize=(40, 2))

# Plot the predicted values against the actual values
plt.scatter(knn_results.loc[y1_test.index, 'Fecha_hora'], y1_test, color='blue', label='Actual')
plt.scatter(knn_results.loc[y1_test.index, 'Fecha_hora'], y1_pred_lr, color='red', label='Predicted')

# Set labels and title
plt.xlabel('DateTime')
plt.ylabel('Anomaly')
plt.title('Linear Regression - Actual vs Predicted')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Add legend
plt.legend()

# Display the plot
plt.show()

# GRADIENT BOOSTING MODEL

from sklearn.ensemble import GradientBoostingRegressor
# Initializing the gradient boosting regressor
gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Training the model
gradient_boosting.fit(X1_train, y1_train)

# Predicting on the test set
y1_pred_gb = gradient_boosting.predict(X1_test)

# Evaluate the model
mse_gb_1 = mean_squared_error(y1_test, y1_pred_gb)
r2_gb_1 = r2_score(y1_test, y1_pred_gb)
mae_gb_1 = mean_absolute_error(y1_test, y1_pred_gb)

# Print the evaluation metrics
print("Mean Absolute Error (MAE):", mae_gb_1)
print('Mean Squared Error (MSE):', mse_gb_1)
print('Coefficient of Determination (R^2):', r2_gb_1)

y1_pred_gb_binary = (y1_pred_gb >= 0.6).astype(int)

# Calculate accuracy, precision and recall
accuracy_gb_1 = accuracy_score(y1_test, y1_pred_gb_binary)
precision_gb_1 = precision_score(y1_test, y1_pred_gb_binary)
recall_gb_1 = recall_score(y1_test, y1_pred_gb_binary)

print("Accuracy:", accuracy_gb_1)
print("Precision:", precision_gb_1)
print("Recall:", recall_gb_1)

# RECURRENT NEURAL NETOWORKS(RNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assuming X1_train, y1_train, X1_test, and y1_test are your training and testing data

# Convert the DataFrames to NumPy arrays and reshape the input data
X1_train_rnn_1 = X1_train.values.reshape(X1_train.shape[0], X1_train.shape[1], 1)
X1_test_rnn_1 = X1_test.values.reshape(X1_test.shape[0], X1_test.shape[1], 1)

# Define the RNN model
model = Sequential()
model.add(SimpleRNN(units=32, input_shape=(X1_train_rnn_1.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X1_train_rnn_1, y1_train, epochs=5, batch_size=32)

# Make predictions on the test data
y1_pred_rnn_1 = model.predict(X1_test_rnn_1)

# Reshape the predicted values
y1_pred_rnn_1 = y1_pred_rnn_1.reshape(-1)

# Calculate evaluation metrics
mse_rnn_1 = mean_squared_error(y1_test, y1_pred_rnn_1)
mae_rnn_1 = mean_absolute_error(y1_test, y1_pred_rnn_1)
r2_rnn_1 = r2_score(y1_test, y1_pred_rnn_1)

# Print the evaluation metrics
print("MSE:", mse_rnn_1)
print("MAE:", mae_rnn_1)
print("R^2:", r2_rnn_1)

# Evaluate the model on the test set
y1_pred_prob_rnn_1 = model.predict(X1_test_rnn_1)
y1_pred_rnn_1 = (y1_pred_prob_rnn_1 > 0.6).astype(int)

# Calculate precision and recall
accuracy_rnn_1 = accuracy_score(y1_test, y1_pred_rnn_1)
precision_rnn_1 = precision_score(y1_test, y1_pred_rnn_1)
recall_rnn_1 = recall_score(y1_test, y1_pred_rnn_1)

print("Accuracy:", accuracy_rnn_1)
print("Precision:", precision_rnn_1)
print("Recall:", recall_rnn_1)



# LONG SHORT TERM MEMORY (LSTM)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import precision_score, recall_score

# Assuming X1_train, y1_train, X1_test, and y1_test are your training and testing data in NumPy array format

# Convert the DataFrame to NumPy array and reshape the input data
X1_train_lstm_1 = X1_train.values.reshape(X1_train.shape[0], X1_train.shape[1], 1)
X1_test_lstm_1 = X1_test.values.reshape(X1_test.shape[0], X1_test.shape[1], 1)

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=32, input_shape=(X1_train.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X1_train_lstm_1, y1_train, epochs=5, batch_size=32)

# Make predictions on the test data
y1_pred_lstm_1 = model.predict(X1_test_lstm_1)

# Reshape the predicted values
y1_pred_lstm_1 = y1_pred_lstm_1.reshape(-1)

# Calculate evaluation metrics
mse_lstm_1 = mean_squared_error(y1_test, y1_pred_lstm_1)
mae_lstm_1 = mean_absolute_error(y1_test, y1_pred_lstm_1)
r2_lstm_1 = r2_score(y1_test, y1_pred_lstm_1)

# Print the evaluation metrics
print("MSE:", mse_lstm_1)
print("MAE:", mae_lstm_1)
print("R^2:", r2_lstm_1)

# Evaluate the model on the test set
y1_pred_prob_lstm_1 = model.predict(X1_test_lstm_1)
y1_pred_lstm_1 = (y1_pred_prob_lstm_1 > 0.6).astype(int)

# Calculate precision and recall
accuracy_lstm_1 = accuracy_score(y1_test, y1_pred_lstm_1)
precision_lstm_1 = precision_score(y1_test, y1_pred_lstm_1)
recall_lstm_1 = recall_score(y1_test, y1_pred_lstm_1)

print("Accuracy:", accuracy_lstm_1)
print("Precision:", precision_lstm_1)
print("Recall:", recall_lstm_1)

from sklearn.metrics import confusion_matrix

# Assuming y3_test and y3_pred are your actual and predicted labels, respectively

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y1_test, y1_pred_lstm_1)

print(conf_matrix)

# CONVULTIONAL NEURAL NETWORKS (CNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Assuming X1_train and y1_train are your training data in NumPy array format

# Convert the DataFrame to NumPy array and reshape the input data
X1_train_cnn_1 = X1_train.values.reshape(X1_train.shape[0], X1_train.shape[1], 1)

# Define the CNN model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X1_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X1_train_cnn_1, y1_train, epochs=5, batch_size=32)

# Make predictions on the test data
y1_pred_cnn_1 = model.predict(X1_test)

# Reshape the predicted values
y1_pred_cnn_1 = y1_pred_cnn_1.reshape(-1)

# Calculate evaluation metrics
mse_cnn_1 = mean_squared_error(y1_test, y1_pred_cnn_1)
mae_cnn_1 = mean_absolute_error(y1_test, y1_pred_cnn_1)
r2_cnn_1 = r2_score(y1_test, y1_pred_cnn_1)

# Print the evaluation metrics
print("MSE:", mse_cnn_1)
print("MAE:", mae_cnn_1)
print("R^2:", r2_cnn_1)

# Evaluate the model on the test set
y1_pred_prob_cnn_1 = model.predict(X1_test)
y1_pred_cnn_1 = (y1_pred_prob_cnn_1 > 0.6).astype(int)

# Calculate precision and recall
accuracy_cnn_1 = accuracy_score(y1_test, y1_pred_cnn_1)
precision_cnn_1 = precision_score(y1_test, y1_pred_cnn_1)
recall_cnn_1 = recall_score(y1_test, y1_pred_cnn_1)

print("Accuracy:", accuracy_lstm_1)
print("Precision:", precision_lstm_1)
print("Recall:", recall_lstm_1)

####################################################################################### KNN - "K" NEAREST NEIGHBORS MODEL

knn_results

knn_results = knn_results.sort_values('Fecha_hora')
knn_results = knn_results.reset_index()
knn_results = knn_results.drop('index', axis=1)
#knn_results = knn_results.drop('level_0', axis=1)
knn_results

column_types = knn_results.dtypes
print(column_types)

# Define a new list of row names
new_column_names = ['Fecha_hora','Active_Power', 'Reactive_Power', 'Excitation_Current','Excitation_Voltage','Ua','Ub','Uc','Ia','Ib','Ic','Anomaly','Anomaly_Score']

# Assign the new row names to the DataFrame
#df.index = new_row_names
knn_results.columns = new_column_names

# Iterate over each column
for column in knn_results.columns:
    # Calculate the mean of the column
    mean_value = knn_results[column].mean()

    # Fill missing values with the mean value
    knn_results[column].fillna(mean_value, inplace=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Select only the numeric columns for normalization
numeric_columns = knn_results.select_dtypes(include=[float, int]).columns

# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize each numeric column in the DataFrame using Z-score
knn_results_normalized = knn_results.copy()  # Create a copy of the original DataFrame
knn_results_normalized[numeric_columns] = scaler.fit_transform(knn_results[numeric_columns])

# The 'anomaly' column remains unchanged
knn_results_normalized['Anomaly'] = knn_results['Anomaly']
knn_results_normalized['Fecha_hora'] = knn_results['Fecha_hora']

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

knn_results['year'] = knn_results['Fecha_hora'].dt.year
knn_results['month'] = knn_results['Fecha_hora'].dt.month
knn_results['day'] = knn_results['Fecha_hora'].dt.day
knn_results['hour'] = knn_results['Fecha_hora'].dt.hour
knn_results['minute'] = knn_results['Fecha_hora'].dt.minute

column_name = 'Fecha_hora'
knn_results = knn_results.drop(column_name, axis=1)

knn_results

# Assuming `knn_results` is your dataset
X2 = knn_results.drop('Anomaly', axis=1)  # Features
y2 = knn_results['Anomaly']  # Target variable

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X2_train, y2_train)

# Make predictions on the test data
y2_pred_lr_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_lr_2 = mean_squared_error(y2_test, y2_pred_lr_2)
mae_lr_2 = mean_absolute_error(y2_test, y2_pred_lr_2)
r2_lr_2 = r2_score(y2_test, y2_pred_lr_2)

# Print the evaluation metrics
print("MSE:", mse_lr_2)
print("MAE:", mae_lr_2)
print("R^2:", r2_lr_2)

# Evaluate the model on the test set
y2_pred_prob_lr_2 = model.predict(X2_test)
y2_pred_lr_2_1 = (y2_pred_prob_lr_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_lr_2 = accuracy_score(y2_test, y2_pred_lr_2_1)
precision_lr_2 = precision_score(y2_test, y2_pred_lr_2_1)
recall_lr_2 = recall_score(y2_test, y2_pred_lr_2_1)

print("Accuracy:", accuracy_lr_2)
print("Precision:", precision_lr_2)
print("Recall:", recall_lr_2)

# RANDOM FOREST MODEL

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [50, 100, 200, 300, 400, 500],   [50-500]
#    'max_depth': [5, 10, 15, 20],                    [5-20]
#    'min_samples_split': [2, 5, 8, 10],              [1-10]
#    'min_samples_leaf': [1, 3, 2, 4, 5]              [1-5]
#}

# estimated time to run 4800 times the training process is 3 hours 20 min# (6*4*4*5*10)

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [100, 200, 300],
#    'max_depth': [5, 10, 15],
#    'min_samples_split': [2, 5, 8],
#    'min_samples_leaf': [2, 4, 5]
#}

# Grid search to find the best hyperparameters
#grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
#grid_search.fit(X1_train, y1_train)

# Best hyperparameters and best model
#best_params = grid_search.best_params_
#best_model = grid_search.best_estimator_

# Create a Random Forest classifier
random_forest = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(X2_train, y2_train)

# Predicting on the test set
y2_pred_rf_2 = random_forest.predict(X2_test)

# Make predictions on the test data
y2_pred_rf_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_rf_2 = mean_squared_error(y2_test, y2_pred_rf_2)
mae_rf_2 = mean_absolute_error(y2_test, y2_pred_rf_2)
r2_rf_2 = r2_score(y2_test, y2_pred_rf_2)

# Print the evaluation metrics
print("MSE:", mse_rf_2)
print("MAE:", mae_rf_2)
print("R^2:", r2_rf_2)

# Evaluate the model on the test set
y2_pred_prob_rf_2 = model.predict(X2_test)
y2_pred_rf_2_1 = (y2_pred_prob_rf_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_rf_2 = accuracy_score(y2_test, y2_pred_rf_2_1)
precision_rf_2 = precision_score(y2_test, y2_pred_rf_2_1)
recall_rf_2 = recall_score(y2_test, y2_pred_rf_2_1)

print("Accuracy:", accuracy_rf_2)
print("Precision:", precision_rf_2)
print("Recall:", recall_rf_2)

# GRADIENT BOOSTING MODEL

from sklearn.ensemble import GradientBoostingRegressor
# Initializing the gradient boosting regressor
gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Training the model
gradient_boosting.fit(X2_train, y2_train)

# Predicting on the test set
y2_pred_gb_2 = gradient_boosting.predict(X2_test)

# Make predictions on the test data
y2_pred_gb_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_gb_2 = mean_squared_error(y2_test, y2_pred_gb_2)
mae_gb_2 = mean_absolute_error(y2_test, y2_pred_gb_2)
r2_gb_2 = r2_score(y2_test, y2_pred_gb_2)

# Print the evaluation metrics
print("MSE:", mse_gb_2)
print("MAE:", mae_gb_2)
print("R^2:", r2_gb_2)

# Evaluate the model on the test set
y2_pred_prob_gb_2 = model.predict(X2_test)
y2_pred_gb_2_1 = (y2_pred_prob_gb_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_gb_2 = accuracy_score(y2_test, y2_pred_gb_2_1)
precision_gb_2 = precision_score(y2_test, y2_pred_gb_2_1)
recall_gb_2 = recall_score(y2_test, y2_pred_gb_2_1)

print("Accuracy:", accuracy_gb_2)
print("Precision:", precision_gb_2)
print("Recall:", recall_gb_2)

# RECURRENT NEURAL NETOWORKS(RNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN

# Assuming X1_train and y1_train are your training data

# Convert the DataFrame to a NumPy array and reshape the input data
X2_train_rnn_2 = X2_train.values.reshape(X2_train.shape[0], X2_train.shape[1], 1)

# Define the RNN model
model = Sequential()
model.add(SimpleRNN(units=32, input_shape=(X2_train.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X2_train_rnn_2, y2_train, epochs=5, batch_size=32)

# Make predictions on the test data
y2_pred_rnn_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_rnn_2 = mean_squared_error(y2_test, y2_pred_rnn_2)
mae_rnn_2 = mean_absolute_error(y2_test, y2_pred_rnn_2)
r2_rnn_2 = r2_score(y2_test, y2_pred_rnn_2)

# Print the evaluation metrics
print("MSE:", mse_rnn_2)
print("MAE:", mae_rnn_2)
print("R^2:", r2_rnn_2)

# Evaluate the model on the test set
y2_pred_prob_rnn_2 = model.predict(X2_test)
y2_pred_rnn_2_1 = (y2_pred_prob_rnn_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_rnn_2 = accuracy_score(y2_test, y2_pred_rnn_2_1)
precision_rnn_2 = precision_score(y2_test, y2_pred_rnn_2_1)
recall_rnn_2 = recall_score(y2_test, y2_pred_rnn_2_1)

print("Accuracy:", accuracy_rnn_2)
print("Precision:", precision_rnn_2)
print("Recall:", recall_rnn_2)

# LONG SHORT TERM MEMORY (LSTM)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import precision_score, recall_score

# Assuming X2_train and y2_train are your training data in NumPy array format

# Convert the DataFrame to a NumPy array
X2_train_array = X2_train.values

# Reshape the input data to match the expected input shape
X2_train_lstm_2 = X2_train_array.reshape(X2_train_array.shape[0], X2_train_array.shape[1], 1)

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=32, input_shape=(X2_train_lstm_2.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X2_train_lstm_2, y2_train, epochs=5, batch_size=32)

# Make predictions on the test data
y2_pred_lstm_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_lstm_2 = mean_squared_error(y2_test, y2_pred_lstm_2)
mae_lstm_2 = mean_absolute_error(y2_test, y2_pred_lstm_2)
r2_lstm_2 = r2_score(y2_test, y2_pred_lstm_2)

# Print the evaluation metrics
print("MSE:", mse_lstm_2)
print("MAE:", mae_lstm_2)
print("R^2:", r2_lstm_2)

# Evaluate the model on the test set
y2_pred_prob_lstm_2 = model.predict(X2_test)
y2_pred_lstm_2_1 = (y2_pred_prob_lstm_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_lstm_2 = accuracy_score(y2_test, y2_pred_lstm_2_1)
precision_lstm_2 = precision_score(y2_test, y2_pred_lstm_2_1)
recall_lstm_2 = recall_score(y2_test, y2_pred_lstm_2_1)

print("Accuracy:", accuracy_lstm_2)
print("Precision:", precision_lstm_2)
print("Recall:", recall_lstm_2)

# CONVULTIONAL NEURAL NETWORKS (CNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from sklearn.metrics import precision_score, recall_score

# Assuming X2_train and y2_train are your training data in NumPy array format

# Convert the DataFrame to a NumPy array
X2_train_array = X2_train.values

# Reshape the input data to match the expected input shape
X2_train_cnn_2 = X2_train_array.reshape(X2_train_array.shape[0], X2_train_array.shape[1], 1)

# Define the CNN model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X2_train_cnn_2.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X2_train_cnn_2, y2_train, epochs=5, batch_size=32)

y2_pred_cnn_2 = model.predict(X2_test)

# Calculate evaluation metrics
mse_cnn_2 = mean_squared_error(y2_test, y2_pred_cnn_2)
mae_cnn_2 = mean_absolute_error(y2_test, y2_pred_cnn_2)
r2_cnn_2 = r2_score(y2_test, y2_pred_cnn_2)

# Print the evaluation metrics
print("MSE:", mse_cnn_2)
print("MAE:", mae_cnn_2)
print("R^2:", r2_cnn_2)

# Evaluate the model on the test set
y2_pred_prob_cnn_2 = model.predict(X2_test)
y2_pred_cnn_2_1 = (y2_pred_prob_cnn_2 > 0.6).astype(int)

# Calculate precision and recall
accuracy_cnn_2 = accuracy_score(y2_test, y2_pred_cnn_2_1)
precision_cnn_2 = precision_score(y2_test, y2_pred_cnn_2_1)
recall_cnn_2 = recall_score(y2_test, y2_pred_cnn_2_1)

print("Accuracy:", accuracy_cnn_2)
print("Precision:", precision_cnn_2)
print("Recall:", recall_cnn_2)

##################################################################################### LOCAL OUTLIER FACTOR MODEL

lof_results = lof_results.sort_values('Fecha_hora')
lof_results = lof_results.reset_index()
lof_results = lof_results.drop('index', axis=1)
# lof_results = lof_results.drop('level_0', axis=1)
lof_results

column_types = knn_results.dtypes
print(column_types)

# Define a new list of row names
new_column_names = ['Fecha_hora','Active_Power', 'Reactive_Power', 'Excitation_Current','Excitation_Voltage','Ua','Ub','Uc','Ia','Ib','Ic','Anomaly','Anomaly_Score']

# Assign the new row names to the DataFrame
#df.index = new_row_names
lof_results.columns = new_column_names

# Iterate over each column
for column in lof_results.columns:
    # Calculate the mean of the column
    mean_value = lof_results[column].mean()

    # Fill missing values with the mean value
    lof_results[column].fillna(mean_value, inplace=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Select only the numeric columns for normalization
numeric_columns = lof_results.select_dtypes(include=[float, int]).columns

# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize each numeric column in the DataFrame using Z-score
lof_results_normalized = lof_results.copy()  # Create a copy of the original DataFrame
lof_results_normalized[numeric_columns] = scaler.fit_transform(lof_results[numeric_columns])

# The 'anomaly' column remains unchanged
lof_results_normalized['Anomaly'] = lof_results['Anomaly']
lof_results_normalized['Fecha_hora'] = lof_results['Fecha_hora']

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

lof_results['year'] = lof_results['Fecha_hora'].dt.year
lof_results['month'] = lof_results['Fecha_hora'].dt.month
lof_results['day'] = lof_results['Fecha_hora'].dt.day
lof_results['hour'] = lof_results['Fecha_hora'].dt.hour
lof_results['minute'] = lof_results['Fecha_hora'].dt.minute

column_name = 'Fecha_hora'
lof_results = lof_results.drop(column_name, axis=1)

# Assuming `lof_results` is your dataset
X3 = lof_results.drop('Anomaly', axis=1)  # Features
y3 = lof_results['Anomaly']  # Target variable

X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X3_train, y3_train)

y3_pred_lr_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_lr_3 = mean_squared_error(y3_test, y3_pred_lr_3)
mae_lr_3 = mean_absolute_error(y3_test, y3_pred_lr_3)
r2_lr_3 = r2_score(y3_test, y3_pred_lr_3)

# Print the evaluation metrics
print("MSE:", mse_lr_3)
print("MAE:", mae_lr_3)
print("R^2:", r2_lr_3)

# Evaluate the model on the test set
y3_pred_prob_lr_3 = model.predict(X3_test)
y3_pred_lr_3_1 = (y3_pred_prob_lr_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_lr_3 = accuracy_score(y3_test, y3_pred_lr_3_1)
precision_lr_3 = precision_score(y3_test, y3_pred_lr_3_1)
recall_lr_3 = recall_score(y3_test, y3_pred_lr_3_1)

print("Accuracy:", accuracy_lr_2)
print("Precision:", precision_lr_2)
print("Recall:", recall_lr_2)

# RANDOM FOREST MODEL

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [50, 100, 200, 300, 400, 500],   [50-500]
#    'max_depth': [5, 10, 15, 20],                    [5-20]
#    'min_samples_split': [2, 5, 8, 10],              [1-10]
#    'min_samples_leaf': [1, 3, 2, 4, 5]              [1-5]
#}

# estimated time to run 4800 times the training process is 3 hours 20 min# (6*4*4*5*10)

# Defining the hyperparameter grid
#param_grid = {
#    'n_estimators': [100, 200, 300],
#    'max_depth': [5, 10, 15],
#    'min_samples_split': [2, 5, 8],
#    'min_samples_leaf': [2, 4, 5]
#}

# Grid search to find the best hyperparameters
#grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
#grid_search.fit(X1_train, y1_train)

# Best hyperparameters and best model
#best_params = grid_search.best_params_
#best_model = grid_search.best_estimator_

# Create a Random Forest classifier
random_forest = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the classifier
random_forest.fit(X3_train, y3_train)

y3_pred_rf_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_rf_3 = mean_squared_error(y3_test, y3_pred_rf_3)
mae_rf_3 = mean_absolute_error(y3_test, y3_pred_rf_3)
r2_rf_3 = r2_score(y3_test, y3_pred_rf_3)

# Print the evaluation metrics
print("MSE:", mse_rf_3)
print("MAE:", mae_rf_3)
print("R^2:", r2_rf_3)

# Evaluate the model on the test set
y3_pred_prob_rf_3 = model.predict(X3_test)
y3_pred_rf_3_1 = (y3_pred_prob_rf_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_rf_3 = accuracy_score(y3_test, y3_pred_rf_3_1)
precision_rf_3 = precision_score(y3_test, y3_pred_rf_3_1)
recall_rf_3 = recall_score(y3_test, y3_pred_rf_3_1)

print("Accuracy:", accuracy_rf_2)
print("Precision:", precision_rf_2)
print("Recall:", recall_rf_2)

# GRADIENT BOOSTING MODEL

from sklearn.ensemble import GradientBoostingRegressor
# Initializing the gradient boosting regressor
gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Training the model
gradient_boosting.fit(X3_train, y3_train)

# Predicting on the test set
y3_pred_gb = gradient_boosting.predict(X3_test)

y3_pred_gb_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_gb_3 = mean_squared_error(y3_test, y3_pred_gb_3)
mae_gb_3 = mean_absolute_error(y3_test, y3_pred_gb_3)
r2_gb_3 = r2_score(y3_test, y3_pred_gb_3)

# Print the evaluation metrics
print("MSE:", mse_gb_3)
print("MAE:", mae_gb_3)
print("R^2:", r2_gb_3)

# Evaluate the model on the test set
y3_pred_prob_gb_3 = model.predict(X3_test)
y3_pred_gb_3_1 = (y3_pred_prob_gb_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_gb_3 = accuracy_score(y3_test, y3_pred_gb_3_1)
precision_gb_3 = precision_score(y3_test, y3_pred_gb_3_1)
recall_gb_3 = recall_score(y3_test, y3_pred_gb_3_1)

print("Accuracy:", accuracy_gb_2)
print("Precision:", precision_gb_2)
print("Recall:", recall_gb_2)

# RECURRENT NEURAL NETOWORKS(RNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN

# Assuming X1_train and y1_train are your training data

# Convert the DataFrame to a NumPy array and reshape the input data
X3_train_rnn_3 = X3_train.values.reshape(X3_train.shape[0], X3_train.shape[1], 1)

# Define the RNN model
model = Sequential()
model.add(SimpleRNN(units=32, input_shape=(X3_train_rnn_3.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X3_train_rnn_3, y3_train, epochs=5, batch_size=32)

y3_pred_rnn_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_rnn_3 = mean_squared_error(y3_test, y3_pred_rnn_3)
mae_rnn_3 = mean_absolute_error(y3_test, y3_pred_rnn_3)
r2_rnn_3 = r2_score(y3_test, y3_pred_rnn_3)

# Print the evaluation metrics
print("MSE:", mse_rnn_3)
print("MAE:", mae_rnn_3)
print("R^2:", r2_rnn_3)

# Evaluate the model on the test set
y3_pred_prob_rnn_3 = model.predict(X3_test)
y3_pred_rnn_3_1 = (y3_pred_prob_rnn_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_rnn_3 = accuracy_score(y3_test, y3_pred_rnn_3_1)
precision_rnn_3 = precision_score(y3_test, y3_pred_rnn_3_1)
recall_rnn_3 = recall_score(y3_test, y3_pred_rnn_3_1)

print("Accuracy:", accuracy_rnn_2)
print("Precision:", precision_rnn_2)
print("Recall:", recall_rnn_2)

# LONG SHORT TERM MEMORY (LSTM)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import precision_score, recall_score

# Assuming X3_train and y3_train are your training data in NumPy array format

# Convert the DataFrame to a NumPy array
X3_train_array = X3_train.values

# Reshape the input data to match the expected input shape
X3_train_lstm_3 = X3_train_array.reshape(X3_train_array.shape[0], X3_train_array.shape[1], 1)

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=32, input_shape=(X3_train_lstm_3.shape[1], 1)))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X3_train_lstm_3, y3_train, epochs=5, batch_size=32)

y3_pred_lstm_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_lstm_3 = mean_squared_error(y3_test, y3_pred_lstm_3)
mae_lstm_3 = mean_absolute_error(y3_test, y3_pred_lstm_3)
r2_lstm_3 = r2_score(y3_test, y3_pred_lstm_3)

# Print the evaluation metrics
print("MSE:", mse_lstm_3)
print("MAE:", mae_lstm_3)
print("R^2:", r2_lstm_3)

# Evaluate the model on the test set
y3_pred_prob_lstm_3 = model.predict(X3_test)
y3_pred_lstm_3_1 = (y3_pred_prob_lstm_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_lstm_3 = accuracy_score(y3_test, y3_pred_lstm_3_1)
precision_lstm_3 = precision_score(y3_test, y3_pred_lstm_3_1)
recall_lstm_3 = recall_score(y3_test, y3_pred_lstm_3_1)

print("Accuracy:", accuracy_lstm_3)
print("Precision:", precision_lstm_3)
print("Recall:", recall_lstm_3)

# CONVULTIONAL NEURAL NETWORKS (CNNs)

import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from sklearn.metrics import precision_score, recall_score, accuracy_score

# Assuming X3_train and y3_train are your training data in NumPy array format

# Convert the DataFrame to a NumPy array
X3_train_array = X3_train.values

# Reshape the input data to match the expected input shape
X3_train_cnn_3 = X3_train_array.reshape(X3_train_array.shape[0], X3_train_array.shape[1], 1)

# Define the CNN model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X3_train_cnn_3.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X3_train_cnn_3, y3_train, epochs=5, batch_size=32)

y3_pred_cnn_3 = model.predict(X3_test)

# Calculate evaluation metrics
mse_cnn_3 = mean_squared_error(y3_test, y3_pred_cnn_3)
mae_cnn_3 = mean_absolute_error(y3_test, y3_pred_cnn_3)
r2_cnn_3 = r2_score(y3_test, y3_pred_cnn_3)

# Print the evaluation metrics
print("MSE:", mse_cnn_3)
print("MAE:", mae_cnn_3)
print("R^2:", r2_cnn_3)

# Evaluate the model on the test set
y3_pred_prob_cnn_3 = model.predict(X3_test)
y3_pred_cnn_3_1 = (y3_pred_prob_cnn_3 > 0.6).astype(int)

# Calculate precision and recall
accuracy_cnn_3 = accuracy_score(y3_test, y3_pred_cnn_3_1)
precision_cnn_3 = precision_score(y3_test, y3_pred_cnn_3_1)
recall_cnn_3 = recall_score(y3_test, y3_pred_cnn_3_1)

print("Accuracy:", accuracy_cnn_3)
print("Precision:", precision_cnn_3)
print("Recall:", recall_cnn_3)

######## Resume Table #########

# import pandas and matplotlib
import pandas as pd
import matplotlib.pyplot as plt

# create 2D array of table given above
data = [
        ['Isolation_Forest','Linear Regression',          mae_lr_1   , mse_lr_1, r2_lr_1, accuracy_lr_1, precision_lr_1, recall_lr_1],
        ['Isolation_Forest','Random Forest',              mae_rf_1   , mse_rf_1, r2_lr_1, accuracy_rf_1, precision_rf_1, recall_rf_1  ],
        ['Isolation_Forest','Gradient Boosting',          mae_gb_1   , mse_gb_1, r2_lr_1, accuracy_gb_1, precision_gb_1, recall_gb_1  ],
        ['Isolation_Forest','Recurrent Neural Networks',  mae_rnn_1  , mse_rnn_1, r2_lr_1, accuracy_rnn_1, precision_rnn_1, recall_rnn_1  ],
        ['Isolation_Forest','Long-Short Term Memory',     mae_lstm_1 , mse_lstm_1, r2_lstm_1, accuracy_lstm_1, precision_lstm_1, recall_lstm_1  ],
        ['Isolation_Forest','Concurrent Neural Networks', mae_cnn_1  , mse_cnn_1, r2_lr_1, accuracy_cnn_1, precision_cnn_1, recall_cnn_1  ],
        ['K-Nearest_Neightbor','Linear Regression',          mae_lr_2   , mse_lr_2, r2_lr_2, accuracy_lr_2, precision_lr_2, recall_lr_2],
        ['K-Nearest_Neightbor','Random Forest',              mae_rf_2   , mse_rf_2, r2_lr_2, accuracy_rf_2, precision_rf_2, recall_rf_2  ],
        ['K-Nearest_Neightbor','Gradient Boosting',          mae_gb_2   , mse_gb_2, r2_lr_2, accuracy_gb_2, precision_gb_2, recall_gb_2  ],
        ['K-Nearest_Neightbor','Recurrent Neural Networks',  mae_rnn_2  , mse_rnn_2, r2_lr_2, accuracy_rnn_2, precision_rnn_2, recall_rnn_2  ],
        ['K-Nearest_Neightbor','Long-Short Term Memory',     mae_lstm_2 , mse_lstm_2, r2_lstm_2, accuracy_lstm_2, precision_lstm_2, recall_lstm_2  ],
        ['K-Nearest_Neightbor','Concurrent Neural Networks', mae_cnn_2  , mse_cnn_2, r2_lr_2, accuracy_cnn_2, precision_cnn_2, recall_cnn_2  ],
        ['Local Outlier Factor','Linear Regression',          mae_lr_3   , mse_lr_3, r2_lr_3, accuracy_lr_3, precision_lr_3, recall_lr_3],
        ['Local Outlier Factor','Random Forest',              mae_rf_3   , mse_rf_3, r2_lr_3, accuracy_rf_3, precision_rf_3, recall_rf_3  ],
        ['Local Outlier Factor','Gradient Boosting',          mae_gb_3   , mse_gb_3, r2_lr_3, accuracy_gb_3, precision_gb_3, recall_gb_3  ],
        ['Local Outlier Factor','Recurrent Neural Networks',  mae_rnn_3  , mse_rnn_3, r2_lr_3, accuracy_rnn_3, precision_rnn_3, recall_rnn_3  ],
        ['Local Outlier Factor','Long-Short Term Memory',     mae_lstm_3 , mse_lstm_3, r2_lstm_3, accuracy_lstm_3, precision_lstm_3, recall_lstm_3  ],
        ['Local Outlier Factor','Concurrent Neural Networks', mae_cnn_3  , mse_cnn_3, r2_lr_3, accuracy_cnn_3, precision_cnn_3, recall_cnn_3  ]
        ]

# dataframe created with
# the above data array
table_resume = pd.DataFrame(data, columns = ['Unsupervised_Model','Supervised_Model', 'MAE',
                                    'MSE', 'R^2',
                                    'Accuracy', 'Precision','Recall'] )


# Rank the DataFrame based on different conditions
table_resume['Rank1'] = table_resume['MAE'].rank(method='dense', ascending=True)
table_resume['Rank2'] = table_resume['MSE'].rank(method='dense', ascending=True)
table_resume['Rank3'] = table_resume['Precision'].rank(method='dense', ascending=False)
table_resume['Rank4'] = table_resume['Recall'].rank(method='dense', ascending=False)

table_resume

from sklearn.metrics import confusion_matrix

# Assuming y3_test and y3_pred are your actual and predicted labels, respectively

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y3_test, y3_pred)

print(conf_matrix)

















